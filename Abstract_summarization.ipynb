{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu4iKM43trnKNzU3uf/+5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhapurahet/AI_Concepts_Projects/blob/main/Abstract_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Arxiv Project (arxiv is an open source online collection of articles)\n",
        "\n",
        "--> This is the demonstration of how a text can be summarized through the help of inbuild models available on Hugging Face platform."
      ],
      "metadata": {
        "id": "87g28C8pisEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv\n",
        "!pip install transformers\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0drpYbzri8WS",
        "outputId": "28d8f150-d5df-4a71-8816-d54fafb6602f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.11/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8BkuNBD_jEla"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to fetch the AI related papers\n",
        "query = \"ai OR Artificial Intelligence OR Machine Learning OR Deep Learning\"\n",
        "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "# Fetch papers\n",
        "papers = []\n",
        "for result in search.results():\n",
        "  papers.append({\n",
        "      \"published\": result.published,\n",
        "      \"title\": result.title,\n",
        "      \"abstract\": result.summary,\n",
        "      \"categories\": result.categories\n",
        "  })\n",
        "\n",
        "# Converting the papers to dataframe (dataframe will organize the data in rows and columns)\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "df.head(10)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "jvzOTm6SjYTP",
        "outputId": "b8309bdd-8af6-49d6-f477-b46f050315ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-69c4983eaa03>:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2025-04-30 17:59:02+00:00   \n",
              "1 2025-04-30 17:58:06+00:00   \n",
              "2 2025-04-30 17:57:22+00:00   \n",
              "3 2025-04-30 17:56:23+00:00   \n",
              "4 2025-04-30 17:55:48+00:00   \n",
              "5 2025-04-30 17:55:29+00:00   \n",
              "6 2025-04-30 17:55:24+00:00   \n",
              "7 2025-04-30 17:53:08+00:00   \n",
              "8 2025-04-30 17:51:20+00:00   \n",
              "9 2025-04-30 17:50:47+00:00   \n",
              "\n",
              "                                                                                                               title  \\\n",
              "0                                                                           A Survey of Interactive Generative Video   \n",
              "1                            TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments   \n",
              "2                                                  COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning   \n",
              "3                        Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support   \n",
              "4                                                              Characterizing AI Agents for Alignment and Governance   \n",
              "5                                               Differentiable Room Acoustic Rendering with Multi-View Vision Priors   \n",
              "6                                           Active Light Modulation to Counter Manipulation of Speech Visual Content   \n",
              "7  Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks   \n",
              "8                   Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic   \n",
              "9                                   Parameter Inference of Black Hole Images using Deep Learning in Visibility Space   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             abstract  \\\n",
              "0  Interactive Generative Video (IGV) has emerged as a crucial technology in\\nresponse to the growing demand for high-quality, interactive video content\\nacross various domains. In this paper, we define IGV as a technology that\\ncombines generative capabilities to produce diverse high-quality video content\\nwith interactive features that enable user engagement through control signals\\nand responsive feedback. We survey the current landscape of IGV applications,\\nfocusing on three major domains: 1) gaming, where IGV enables infinite\\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\\nphysics-aware environment synthesizer for training agents in multimodal\\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\\nIGV provides closed-loop simulation capabilities for safety-critical testing\\nand validation. To guide future development, we propose a comprehensive\\nframework that decomposes an ideal IGV system into five essential modules:\\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\\nsystematically analyze the technical challenges and future directions in\\nrealizing each component for an ideal IGV system, such as achieving real-time\\ngeneration, enabling open-domain control, maintaining long-term coherence,\\nsimulating accurate physics, and integrating causal reasoning. We believe that\\nthis systematic analysis will facilitate future research and development in the\\nfield of IGV, ultimately advancing the technology toward more sophisticated and\\npractical applications.   \n",
              "1                                                                                                                                                    Objectives: While Large Language Models (LLMs) have been widely used to\\nassist clinicians and support patients, no existing work has explored dialogue\\nsystems for standard diagnostic interviews and assessments. This study aims to\\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\\ndialogue system that replicates clinician behavior. Materials and Methods: We\\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\\nDialogue Acts schema specifically designed for clinical interviews.\\nAdditionally, we develop a patient simulation approach based on real-life\\ninterview transcripts to replace time-consuming and costly manual testing by\\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\\nassess the dialogue system from both the agent and patient simulation\\nperspectives. Expert evaluations by conversation and clinical specialists show\\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\\nOur system performs at the level of average clinicians, with room for future\\nenhancements in communication styles and response appropriateness. Conclusions:\\nOur TRUST framework shows its potential to facilitate mental healthcare\\navailability.   \n",
              "2                                                                                                                                                           Multimodal Large Language Models (MLLMs) excel at simple vision-language\\ntasks but struggle when faced with complex tasks that require multiple\\ncapabilities, such as simultaneously recognizing objects, counting them, and\\nunderstanding their spatial relationships. This might be partially the result\\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\\nMLLMs, has traditionally focused on scaling data volume, but not the\\ncompositional complexity of training examples. We propose COMPACT\\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\\ntraining dataset explicitly controlling for the compositional complexity of the\\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\\nof atomic capabilities to learn complex capabilities more efficiently. Across\\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\\nwhile using less than 10% of its data budget, and even outperforms it on\\nseveral, especially those involving complex multi-capability tasks. For\\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\\nquestions that require four or more atomic capabilities. COMPACT offers a\\nscalable, data-efficient, visual compositional tuning recipe to improve on\\ncomplex visual-language tasks.   \n",
              "3                                                                              Governance institutions must respond to societal risks, including those posed\\nby generative AI. This study empirically examines how public trust in\\ninstitutions and AI technologies, along with perceived risks, shape preferences\\nfor AI regulation. Using the nationally representative 2023 Artificial\\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\\ngovernment, AI companies, and AI technologies, as well as public support for\\nregulatory measures such as slowing AI development or outright bans on advanced\\nAI. Our findings reveal broad public support for AI regulation, with risk\\nperception playing a significant role in shaping policy preferences.\\nIndividuals with higher trust in government favor regulation, while those with\\ngreater trust in AI companies and AI technologies are less inclined to support\\nrestrictions. Trust in government and perceived risks significantly predict\\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\\nAI systems) regulatory interventions. These results highlight the importance of\\npublic opinion in AI governance. As AI capabilities advance, effective\\nregulation will require balancing public concerns about risks with trust in\\ninstitutions. This study provides a foundational empirical baseline for\\npolicymakers navigating AI governance and underscores the need for further\\nresearch into public trust, risk perception, and regulatory strategies in the\\nevolving AI landscape.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                         The creation of effective governance mechanisms for AI agents requires a\\ndeeper understanding of their core properties and how these properties relate\\nto questions surrounding the deployment and operation of agents in the world.\\nThis paper provides a characterization of AI agents that focuses on four\\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\\ndifferent gradations for each dimension, and argue that each dimension raises\\nunique questions about the design, operation, and governance of these systems.\\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\\ntechnical and non-technical governance challenges posed by different classes of\\nAI agents, ranging from narrow task-specific assistants to highly autonomous\\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\\nthis framework provides developers, policymakers, and members of the public\\nwith the opportunity to develop governance approaches that better align with\\ncollective societal goals.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          An immersive acoustic experience enabled by spatial audio is just as crucial\\nas the visual aspect in creating realistic virtual environments. However,\\nexisting methods for room impulse response estimation rely either on\\ndata-demanding learning-based models or computationally expensive physics-based\\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\\nRendering (AV-DAR), a framework that leverages visual cues extracted from\\nmulti-view images and acoustic beam tracing for physics-based room acoustic\\nrendering. Experiments across six real-world environments from two datasets\\ndemonstrate that our multimodal, physics-based approach is efficient,\\ninterpretable, and accurate, significantly outperforming a series of prior\\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\\ncomparable performance to models trained on 10 times more data while delivering\\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale.   \n",
              "6                                                      High-profile speech videos are prime targets for falsification, owing to\\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\\nand unobtrusive system for protecting live speech videos from visual\\nfalsification of speaker identity and lip and facial motion. Unlike predominant\\nfalsification detection methods operating in the digital domain, Spotlight\\ncreates dynamic physical signatures at the event site and embeds them into all\\nvideo recordings via imperceptible modulated light. These physical signatures\\nencode semantically-meaningful features unique to the speech event, including\\nthe speaker's identity and facial motion, and are cryptographically-secured to\\nprevent spoofing. The signatures can be extracted from any video downstream and\\nvalidated against the portrayed speech content to check its integrity. Key\\nelements of Spotlight include (1) a framework for generating extremely compact\\n(i.e., 150-bit), pose-invariant speech video features, based on\\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\\n>200 bps into video while remaining imperceptible both in video and live.\\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\\nvideos. Further, Spotlight is highly robust across recording conditions, video\\npost-processing techniques, and white-box adversarial attacks on its video\\nfeature extraction methodologies.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                      The growing luminosity frontier at the Large Hadron Collider is challenging\\nthe reconstruction and analysis of particle collision events. Increased\\nparticle multiplicities are straining latency and storage requirements at the\\ndata acquisition stage, while new complications are emerging, including higher\\nbackground levels and more frequent particle vertex misassociations. This in\\nturn necessitates the development of more holistic and scalable reconstruction\\nmethods that take advantage of recent advances in machine learning. We propose\\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\\nrepresentations for diverse particle collision relationships and integrated\\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\\nbeauty hadron reconstruction performance. Notably, it concurrently performs\\nparticle vertex association and graph pruning within a single framework. We\\nquantify reconstruction and pruning performance, demonstrate enhanced inference\\ntime scaling with event complexity, and mitigate potential performance loss\\nusing a weighted message passing scheme.   \n",
              "8                                                                                                                                                                                                                                                                                                                                            Neural network-based policies have demonstrated success in many robotic\\napplications, but often lack human-explanability, which poses challenges in\\nsafety-critical deployments. To address this, we propose a neuro-symbolic\\nexplanation framework that generates a weighted signal temporal logic (wSTL)\\nspecification to describe a robot policy in a interpretable form. Existing\\nmethods typically produce explanations that are verbose and inconsistent, which\\nhinders explainability, and loose, which do not give meaningful insights into\\nthe underlying policy. We address these issues by introducing a simplification\\nprocess consisting of predicate filtering, regularization, and iterative\\npruning. We also introduce three novel explainability evaluation metrics --\\nconciseness, consistency, and strictness -- to assess explanation quality\\nbeyond conventional classification metrics. Our method is validated in three\\nsimulated robotic environments, where it outperforms baselines in generating\\nconcise, consistent, and strict wSTL explanations without sacrificing\\nclassification accuracy. This work bridges policy learning with formal methods,\\ncontributing to safer and more transparent decision-making in robotics.   \n",
              "9                                                                                                                                                         Using very long baseline interferometry, the Event Horizon Telescope (EHT)\\ncollaboration has resolved the shadows of two supermassive black holes. Model\\ncomparison is traditionally performed in image space, where imaging algorithms\\nintroduce uncertainties in the recovered structure. Here, we develop a deep\\nlearning framework to perform parameter inference in visibility space, directly\\nusing the data measured by the interferometer without introducing potential\\nerrors and biases from image reconstruction. First, we train and validate our\\nframework on synthetic data derived from general relativistic\\nmagnetohydrodynamics (GRMHD) simulations that vary in magnetic field state,\\nspin, and $R_\\mathrm{high}$. Applying these models to the real data obtained\\nduring the 2017 EHT campaign, and only considering total intensity, we do not\\nderive meaningful constraints on either of these parameters. At present, our\\nmethod is limited both by theoretical uncertainties in the GRMHD simulations\\nand variation between snapshots of the same underlying physical model. However,\\nwe demonstrate that spin and $R_\\mathrm{high}$ could be recovered using this\\nframework through continuous monitoring of our sources, which mitigates\\nvariations due to turbulence. In future work, we anticipate that including\\nspectral or polarimetric information will greatly improve the performance of\\nthis framework.   \n",
              "\n",
              "                         categories  \n",
              "0                           [cs.CV]  \n",
              "1                    [cs.CL, cs.AI]  \n",
              "2                           [cs.CV]  \n",
              "3             [cs.CY, cs.AI, cs.HC]  \n",
              "4    [cs.CY, cs.AI, cs.SY, eess.SY]  \n",
              "5                    [cs.CV, cs.SD]  \n",
              "6             [cs.CV, cs.AI, cs.CR]  \n",
              "7  [physics.data-an, cs.LG, hep-ex]  \n",
              "8                    [cs.RO, cs.FL]  \n",
              "9                     [astro-ph.GA]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-545f58bd-38e4-45b1-bab0-ab8b3d1b8c5f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-04-30 17:59:02+00:00</td>\n",
              "      <td>A Survey of Interactive Generative Video</td>\n",
              "      <td>Interactive Generative Video (IGV) has emerged as a crucial technology in\\nresponse to the growing demand for high-quality, interactive video content\\nacross various domains. In this paper, we define IGV as a technology that\\ncombines generative capabilities to produce diverse high-quality video content\\nwith interactive features that enable user engagement through control signals\\nand responsive feedback. We survey the current landscape of IGV applications,\\nfocusing on three major domains: 1) gaming, where IGV enables infinite\\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\\nphysics-aware environment synthesizer for training agents in multimodal\\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\\nIGV provides closed-loop simulation capabilities for safety-critical testing\\nand validation. To guide future development, we propose a comprehensive\\nframework that decomposes an ideal IGV system into five essential modules:\\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\\nsystematically analyze the technical challenges and future directions in\\nrealizing each component for an ideal IGV system, such as achieving real-time\\ngeneration, enabling open-domain control, maintaining long-term coherence,\\nsimulating accurate physics, and integrating causal reasoning. We believe that\\nthis systematic analysis will facilitate future research and development in the\\nfield of IGV, ultimately advancing the technology toward more sophisticated and\\npractical applications.</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-04-30 17:58:06+00:00</td>\n",
              "      <td>TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments</td>\n",
              "      <td>Objectives: While Large Language Models (LLMs) have been widely used to\\nassist clinicians and support patients, no existing work has explored dialogue\\nsystems for standard diagnostic interviews and assessments. This study aims to\\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\\ndialogue system that replicates clinician behavior. Materials and Methods: We\\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\\nDialogue Acts schema specifically designed for clinical interviews.\\nAdditionally, we develop a patient simulation approach based on real-life\\ninterview transcripts to replace time-consuming and costly manual testing by\\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\\nassess the dialogue system from both the agent and patient simulation\\nperspectives. Expert evaluations by conversation and clinical specialists show\\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\\nOur system performs at the level of average clinicians, with room for future\\nenhancements in communication styles and response appropriateness. Conclusions:\\nOur TRUST framework shows its potential to facilitate mental healthcare\\navailability.</td>\n",
              "      <td>[cs.CL, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-04-30 17:57:22+00:00</td>\n",
              "      <td>COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning</td>\n",
              "      <td>Multimodal Large Language Models (MLLMs) excel at simple vision-language\\ntasks but struggle when faced with complex tasks that require multiple\\ncapabilities, such as simultaneously recognizing objects, counting them, and\\nunderstanding their spatial relationships. This might be partially the result\\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\\nMLLMs, has traditionally focused on scaling data volume, but not the\\ncompositional complexity of training examples. We propose COMPACT\\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\\ntraining dataset explicitly controlling for the compositional complexity of the\\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\\nof atomic capabilities to learn complex capabilities more efficiently. Across\\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\\nwhile using less than 10% of its data budget, and even outperforms it on\\nseveral, especially those involving complex multi-capability tasks. For\\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\\nquestions that require four or more atomic capabilities. COMPACT offers a\\nscalable, data-efficient, visual compositional tuning recipe to improve on\\ncomplex visual-language tasks.</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-04-30 17:56:23+00:00</td>\n",
              "      <td>Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support</td>\n",
              "      <td>Governance institutions must respond to societal risks, including those posed\\nby generative AI. This study empirically examines how public trust in\\ninstitutions and AI technologies, along with perceived risks, shape preferences\\nfor AI regulation. Using the nationally representative 2023 Artificial\\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\\ngovernment, AI companies, and AI technologies, as well as public support for\\nregulatory measures such as slowing AI development or outright bans on advanced\\nAI. Our findings reveal broad public support for AI regulation, with risk\\nperception playing a significant role in shaping policy preferences.\\nIndividuals with higher trust in government favor regulation, while those with\\ngreater trust in AI companies and AI technologies are less inclined to support\\nrestrictions. Trust in government and perceived risks significantly predict\\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\\nAI systems) regulatory interventions. These results highlight the importance of\\npublic opinion in AI governance. As AI capabilities advance, effective\\nregulation will require balancing public concerns about risks with trust in\\ninstitutions. This study provides a foundational empirical baseline for\\npolicymakers navigating AI governance and underscores the need for further\\nresearch into public trust, risk perception, and regulatory strategies in the\\nevolving AI landscape.</td>\n",
              "      <td>[cs.CY, cs.AI, cs.HC]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-04-30 17:55:48+00:00</td>\n",
              "      <td>Characterizing AI Agents for Alignment and Governance</td>\n",
              "      <td>The creation of effective governance mechanisms for AI agents requires a\\ndeeper understanding of their core properties and how these properties relate\\nto questions surrounding the deployment and operation of agents in the world.\\nThis paper provides a characterization of AI agents that focuses on four\\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\\ndifferent gradations for each dimension, and argue that each dimension raises\\nunique questions about the design, operation, and governance of these systems.\\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\\ntechnical and non-technical governance challenges posed by different classes of\\nAI agents, ranging from narrow task-specific assistants to highly autonomous\\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\\nthis framework provides developers, policymakers, and members of the public\\nwith the opportunity to develop governance approaches that better align with\\ncollective societal goals.</td>\n",
              "      <td>[cs.CY, cs.AI, cs.SY, eess.SY]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-04-30 17:55:29+00:00</td>\n",
              "      <td>Differentiable Room Acoustic Rendering with Multi-View Vision Priors</td>\n",
              "      <td>An immersive acoustic experience enabled by spatial audio is just as crucial\\nas the visual aspect in creating realistic virtual environments. However,\\nexisting methods for room impulse response estimation rely either on\\ndata-demanding learning-based models or computationally expensive physics-based\\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\\nRendering (AV-DAR), a framework that leverages visual cues extracted from\\nmulti-view images and acoustic beam tracing for physics-based room acoustic\\nrendering. Experiments across six real-world environments from two datasets\\ndemonstrate that our multimodal, physics-based approach is efficient,\\ninterpretable, and accurate, significantly outperforming a series of prior\\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\\ncomparable performance to models trained on 10 times more data while delivering\\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale.</td>\n",
              "      <td>[cs.CV, cs.SD]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-04-30 17:55:24+00:00</td>\n",
              "      <td>Active Light Modulation to Counter Manipulation of Speech Visual Content</td>\n",
              "      <td>High-profile speech videos are prime targets for falsification, owing to\\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\\nand unobtrusive system for protecting live speech videos from visual\\nfalsification of speaker identity and lip and facial motion. Unlike predominant\\nfalsification detection methods operating in the digital domain, Spotlight\\ncreates dynamic physical signatures at the event site and embeds them into all\\nvideo recordings via imperceptible modulated light. These physical signatures\\nencode semantically-meaningful features unique to the speech event, including\\nthe speaker's identity and facial motion, and are cryptographically-secured to\\nprevent spoofing. The signatures can be extracted from any video downstream and\\nvalidated against the portrayed speech content to check its integrity. Key\\nelements of Spotlight include (1) a framework for generating extremely compact\\n(i.e., 150-bit), pose-invariant speech video features, based on\\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\\n&gt;200 bps into video while remaining imperceptible both in video and live.\\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\\nvideos. Further, Spotlight is highly robust across recording conditions, video\\npost-processing techniques, and white-box adversarial attacks on its video\\nfeature extraction methodologies.</td>\n",
              "      <td>[cs.CV, cs.AI, cs.CR]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-04-30 17:53:08+00:00</td>\n",
              "      <td>Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks</td>\n",
              "      <td>The growing luminosity frontier at the Large Hadron Collider is challenging\\nthe reconstruction and analysis of particle collision events. Increased\\nparticle multiplicities are straining latency and storage requirements at the\\ndata acquisition stage, while new complications are emerging, including higher\\nbackground levels and more frequent particle vertex misassociations. This in\\nturn necessitates the development of more holistic and scalable reconstruction\\nmethods that take advantage of recent advances in machine learning. We propose\\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\\nrepresentations for diverse particle collision relationships and integrated\\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\\nbeauty hadron reconstruction performance. Notably, it concurrently performs\\nparticle vertex association and graph pruning within a single framework. We\\nquantify reconstruction and pruning performance, demonstrate enhanced inference\\ntime scaling with event complexity, and mitigate potential performance loss\\nusing a weighted message passing scheme.</td>\n",
              "      <td>[physics.data-an, cs.LG, hep-ex]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-04-30 17:51:20+00:00</td>\n",
              "      <td>Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic</td>\n",
              "      <td>Neural network-based policies have demonstrated success in many robotic\\napplications, but often lack human-explanability, which poses challenges in\\nsafety-critical deployments. To address this, we propose a neuro-symbolic\\nexplanation framework that generates a weighted signal temporal logic (wSTL)\\nspecification to describe a robot policy in a interpretable form. Existing\\nmethods typically produce explanations that are verbose and inconsistent, which\\nhinders explainability, and loose, which do not give meaningful insights into\\nthe underlying policy. We address these issues by introducing a simplification\\nprocess consisting of predicate filtering, regularization, and iterative\\npruning. We also introduce three novel explainability evaluation metrics --\\nconciseness, consistency, and strictness -- to assess explanation quality\\nbeyond conventional classification metrics. Our method is validated in three\\nsimulated robotic environments, where it outperforms baselines in generating\\nconcise, consistent, and strict wSTL explanations without sacrificing\\nclassification accuracy. This work bridges policy learning with formal methods,\\ncontributing to safer and more transparent decision-making in robotics.</td>\n",
              "      <td>[cs.RO, cs.FL]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-04-30 17:50:47+00:00</td>\n",
              "      <td>Parameter Inference of Black Hole Images using Deep Learning in Visibility Space</td>\n",
              "      <td>Using very long baseline interferometry, the Event Horizon Telescope (EHT)\\ncollaboration has resolved the shadows of two supermassive black holes. Model\\ncomparison is traditionally performed in image space, where imaging algorithms\\nintroduce uncertainties in the recovered structure. Here, we develop a deep\\nlearning framework to perform parameter inference in visibility space, directly\\nusing the data measured by the interferometer without introducing potential\\nerrors and biases from image reconstruction. First, we train and validate our\\nframework on synthetic data derived from general relativistic\\nmagnetohydrodynamics (GRMHD) simulations that vary in magnetic field state,\\nspin, and $R_\\mathrm{high}$. Applying these models to the real data obtained\\nduring the 2017 EHT campaign, and only considering total intensity, we do not\\nderive meaningful constraints on either of these parameters. At present, our\\nmethod is limited both by theoretical uncertainties in the GRMHD simulations\\nand variation between snapshots of the same underlying physical model. However,\\nwe demonstrate that spin and $R_\\mathrm{high}$ could be recovered using this\\nframework through continuous monitoring of our sources, which mitigates\\nvariations due to turbulence. In future work, we anticipate that including\\nspectral or polarimetric information will greatly improve the performance of\\nthis framework.</td>\n",
              "      <td>[astro-ph.GA]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-545f58bd-38e4-45b1-bab0-ab8b3d1b8c5f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-545f58bd-38e4-45b1-bab0-ab8b3d1b8c5f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-545f58bd-38e4-45b1-bab0-ab8b3d1b8c5f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-925ca41c-6572-427a-83c3-797d28c26bc3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-925ca41c-6572-427a-83c3-797d28c26bc3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-925ca41c-6572-427a-83c3-797d28c26bc3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-04-30 17:50:47+00:00\",\n        \"max\": \"2025-04-30 17:59:02+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2025-04-30 17:51:20+00:00\",\n          \"2025-04-30 17:58:06+00:00\",\n          \"2025-04-30 17:55:29+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic\",\n          \"TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments\",\n          \"Differentiable Room Acoustic Rendering with Multi-View Vision Priors\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Neural network-based policies have demonstrated success in many robotic\\napplications, but often lack human-explanability, which poses challenges in\\nsafety-critical deployments. To address this, we propose a neuro-symbolic\\nexplanation framework that generates a weighted signal temporal logic (wSTL)\\nspecification to describe a robot policy in a interpretable form. Existing\\nmethods typically produce explanations that are verbose and inconsistent, which\\nhinders explainability, and loose, which do not give meaningful insights into\\nthe underlying policy. We address these issues by introducing a simplification\\nprocess consisting of predicate filtering, regularization, and iterative\\npruning. We also introduce three novel explainability evaluation metrics --\\nconciseness, consistency, and strictness -- to assess explanation quality\\nbeyond conventional classification metrics. Our method is validated in three\\nsimulated robotic environments, where it outperforms baselines in generating\\nconcise, consistent, and strict wSTL explanations without sacrificing\\nclassification accuracy. This work bridges policy learning with formal methods,\\ncontributing to safer and more transparent decision-making in robotics.\",\n          \"Objectives: While Large Language Models (LLMs) have been widely used to\\nassist clinicians and support patients, no existing work has explored dialogue\\nsystems for standard diagnostic interviews and assessments. This study aims to\\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\\ndialogue system that replicates clinician behavior. Materials and Methods: We\\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\\nDialogue Acts schema specifically designed for clinical interviews.\\nAdditionally, we develop a patient simulation approach based on real-life\\ninterview transcripts to replace time-consuming and costly manual testing by\\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\\nassess the dialogue system from both the agent and patient simulation\\nperspectives. Expert evaluations by conversation and clinical specialists show\\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\\nOur system performs at the level of average clinicians, with room for future\\nenhancements in communication styles and response appropriateness. Conclusions:\\nOur TRUST framework shows its potential to facilitate mental healthcare\\navailability.\",\n          \"An immersive acoustic experience enabled by spatial audio is just as crucial\\nas the visual aspect in creating realistic virtual environments. However,\\nexisting methods for room impulse response estimation rely either on\\ndata-demanding learning-based models or computationally expensive physics-based\\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\\nRendering (AV-DAR), a framework that leverages visual cues extracted from\\nmulti-view images and acoustic beam tracing for physics-based room acoustic\\nrendering. Experiments across six real-world environments from two datasets\\ndemonstrate that our multimodal, physics-based approach is efficient,\\ninterpretable, and accurate, significantly outperforming a series of prior\\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\\ncomparable performance to models trained on 10 times more data while delivering\\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Picking one abstract and summarizing it using a model available in Hugging Face\n",
        "abstract = df['abstract'][1]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarization\n",
        "summarization_result = summarizer(abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB6jj9cRmVIg",
        "outputId": "1295b840-ba11-444f-dd39-bc3b9a438eec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_KwZOoCphs2",
        "outputId": "7fb1624c-288e-4650-8a54-ebf2b0e56290"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'No existing work has explored dialogue systems for standard diagnostic interviews and assessments. We develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real- life clinical interviews.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}