{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ7lOu4BF+N+SJDA20e2kC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhapurahet/AI_Concepts_Projects/blob/Interview_questions_answers_project/Interview_Question_Answers_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Installing Dependencies"
      ],
      "metadata": {
        "id": "dEchDmYJW6lp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qWDzg2NjVhd1"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-core openai pypdf tiktoken aiofiles fastapi uvicorn jinja2 PyPDF2 faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Open AI Api Key"
      ],
      "metadata": {
        "id": "JNNSfRLMYbLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY = \"\"\n"
      ],
      "metadata": {
        "id": "gORAAgJNYJQg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Taking the raw pdf as input file and extracting the content from the file"
      ],
      "metadata": {
        "id": "gCocY3lvdvLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"/content/Interview_Question_input_data.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "II_c4qhaYf8m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Displaying the content of the input file"
      ],
      "metadata": {
        "id": "zFG28DTId9ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ATBRCkGSdmFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Extracting the page content from all the content generated. <br>\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; => The format of the text extracted is : <br> &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Document(metadata={'metadata': 'producer', 'creator': '', 'creationdate':\n",
        "'2023-07-19T22:21:03+09:00', 'moddate': '2023-07-19T22:21:07\n",
        "&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;+09:00', 'trapped': '/False', 'source': '/content/\n",
        "Interview_Question_input_data.pdf', 'total_pages': 42, 'page':\n",
        "31, 'page_label': '26'}, <br>\n",
        "&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;page_content='26\\nSummary for Policymakers\\nSummary for Policymakers Delayed mitigation action will further increase global') <br>\n",
        "      <br>\n",
        "    But we only want the page content as it has relevant content according to our use case. So we are extracting page_content field from every page throughout the input file.  "
      ],
      "metadata": {
        "id": "icWgBT48gaNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_gen = \"\"\n",
        "for page in data:\n",
        "  question_gen += page.page_content"
      ],
      "metadata": {
        "id": "IZkKKPDZf7jI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_gen\n",
        "# type(question_gen) --> type = str"
      ],
      "metadata": {
        "collapsed": true,
        "id": "owN2DHpGgWnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Making chunks of the content extracted <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;=> The need for chunks is because there is specific limit to size of data that can be send to Embedding &model, hence we need to\n",
        "make smaller chunks of data and then it will be sent to embedding model."
      ],
      "metadata": {
        "id": "DH5GBA3BnzQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "splitter_ques_gen = TokenTextSplitter(\n",
        "    model_name = \"gpt-3.5-turbo\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200\n",
        ")"
      ],
      "metadata": {
        "id": "q3kwOuwSl8mQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_ques_gen = splitter_ques_gen.split_text(question_gen)\n",
        "\n",
        "chunk_ques_gen\n",
        "# type(chunk_ques_gen[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pGwnrUxynFeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Changing the individual chunks from string format to document format <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;=> It is recommended to pass the document format to the vector embedding model rather than string format. Hence, each chunk_ques_gen is converted to document format."
      ],
      "metadata": {
        "id": "ms19mzzv2xkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "document_ques_gen = [Document(page_content = t) for t in chunk_ques_gen]\n",
        "\n",
        "document_ques_gen\n",
        "type(document_ques_gen[0])"
      ],
      "metadata": {
        "id": "riK827uhs-hw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter_ans_gen = TokenTextSplitter(\n",
        "    model_name = \"gpt-3.5-turbo\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 100\n",
        ")"
      ],
      "metadata": {
        "id": "6ATEoDTW2bPM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_answer_gen = splitter_ans_gen.split_documents(\n",
        "    document_ques_gen\n",
        ")"
      ],
      "metadata": {
        "id": "sbqnQTof2jor"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_answer_gen\n",
        "# type(document_answer_gen)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4jZsPesT2oTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Passing the document to model to frame it in the Question format <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;=> Here, we are not using vector database as we only want to frame questions from the document, so a generative AI model would do that, but when generating the answer, we would need to store document into vector database through embedding model. Hence, for answer generation, vector database would play a role as it has to find the most correct answer using searching relevant content from vector database according to the prompt given by user."
      ],
      "metadata": {
        "id": "_HF4aVOPTPBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Using the OpenAI model for generating questions from text"
      ],
      "metadata": {
        "id": "D2HUVY2WZ-dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm_ques_gen_pipeline = ChatOpenAI(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    temperature = 0.3\n",
        ")"
      ],
      "metadata": {
        "id": "WkEU_t-SS2vy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Making a prompt_template which will be given to the model for generating questions"
      ],
      "metadata": {
        "id": "o2aI4cMucwe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are responsible for creating questions from the text. Make sure that every important question is covered.\n",
        "You can do this by asking questions about the text below:\n",
        "\n",
        "-------------\n",
        "{text}\n",
        "-------------\n",
        "\n",
        "Create questions that will make the user more knowledgeble when he completes your Interview.\n",
        "\n",
        "Questions:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qjuiqBK7aNiy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "Prompt_questions = PromptTemplate(template = prompt_template, input_variables = ['text'])"
      ],
      "metadata": {
        "id": "pmpEzmoHbPi8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Using a concept of refining template in which the questions generated by the model are again given to the model to verify that the generated questions are proper and not irrelevant. Hence, again it is passed to the model with refine_template that modifies questions if necessary"
      ],
      "metadata": {
        "id": "rWkjl4JrdQtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "refine_template = (\"\"\"\n",
        "You are responsible for creating questions from the text. Make sure that every important question is covered.\n",
        "we have received some practice questions to a certain extent: {existing_answer}.\n",
        "We have option to refine existing questions or add new ones (only if necessary) with some more context below.\n",
        "\n",
        "------------\n",
        "{text}\n",
        "------------\n",
        "\n",
        "Given the new context, refine the original questions in English.\n",
        "If the context is not helpful, kindly give the original questions.\n",
        "\n",
        "Questions:\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZIGKmpYWb8OA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_prompt_questions = PromptTemplate(\n",
        "    input_variables = ['existing_answer', 'text'],\n",
        "    template = refine_template,\n",
        ")"
      ],
      "metadata": {
        "id": "t_ojdn07dzRA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline,\n",
        "                                      chain_type = \"refine\",\n",
        "                                      verbose = True,\n",
        "                                      question_prompt = Prompt_questions,\n",
        "                                      refine_prompt = refine_prompt_questions\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "G2aexRrngO13"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = ques_gen_chain.run(document_ques_gen)\n",
        "\n",
        "ques_list = questions.split('\\n')\n",
        "print(ques_list)"
      ],
      "metadata": {
        "id": "PdxFLonrhlTB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Initializing an Embedding model"
      ],
      "metadata": {
        "id": "d6oE2rgw4yZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oeAyXKZlzpFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Storing the vector embeddings in the vector database"
      ],
      "metadata": {
        "id": "RlaUczqT44p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(document_answer_gen, embeddings)"
      ],
      "metadata": {
        "id": "EgMHeno80L58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Initializing the model to generate answers of the questions"
      ],
      "metadata": {
        "id": "FdBMrcBP58Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_answer_gen = ChatOpenAI(temperature = 0.1, model = \"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "ZiX4I42r57t9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Connecting the vector database to the model for generating answers"
      ],
      "metadata": {
        "id": "kdoxPafa-QP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen,\n",
        "                                               chain_type=\"stuff\",\n",
        "                                               retriever=vector_store.as_retriever())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GHVrfxl57EA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> Using for loop to iterate over questions and every question will be given to the model and answer will be the output"
      ],
      "metadata": {
        "id": "9IpHVIsc-mTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer each question and save to a file\n",
        "for question in ques_list:\n",
        "    print(\"Question: \", question)\n",
        "    answer = answer_generation_chain.run(question)\n",
        "    print(\"Answer: \", answer)\n",
        "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
        "    # Save answer to file\n",
        "    with open(\"answers.txt\", \"a\") as f:\n",
        "        f.write(\"Question: \" + question + \"\\\\n\")\n",
        "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
        "        f.write(\"--------------------------------------------------\\\\n\\\\n\")"
      ],
      "metadata": {
        "id": "1Jxzgvy77pxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}